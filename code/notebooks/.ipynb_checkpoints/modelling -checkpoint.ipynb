{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8034b9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "#!pip install ipynb\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV, train_test_split\n",
    "from lightgbm import LGBMClassifier\n",
    "from collections import Counter\n",
    "import time\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# custom-made utility modules\n",
    "import ipynb.fs.full.config as c\n",
    "import ipynb.fs.full.plotting as plotting\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3fca23",
   "metadata": {},
   "source": [
    "# MAIN FUNCTIONS\n",
    "\n",
    "The code for this modelling assignment is modular. At this stage we define the main functions that will be used to perform the task. At a later stage we break down the task in defined sub-tasks where each function is called to execute one action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a0dfec6",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (637447776.py, line 181)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/g3/x85s8z310xjds57gq06b6_r80000gn/T/ipykernel_26452/637447776.py\"\u001b[0;36m, line \u001b[0;32m181\u001b[0m\n\u001b[0;31m    def split_data_for_classification(X,ym ratio=0.25):\u001b[0m\n\u001b[0m                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#### LOAD DATA\n",
    "def load_data(path):\n",
    "    '''\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : Class\n",
    "        Used to create the path to the raw data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train_files : Pandas dataframe\n",
    "    test_files : Pandas dataframe\n",
    "\n",
    "    '''\n",
    "    path_to_data = path.to_data_raw()\n",
    "    # list the csv files in the firectory\n",
    "    csv_files = list(filter(lambda x: '.csv' in x, os.listdir(path_to_data)))\n",
    "    \n",
    "    train_files = pd.read_csv(c.join(path_to_data, csv_files[1]))\n",
    "    test_files = pd.read_csv(c.join(path_to_data, csv_files[0]))\n",
    "    \n",
    "    return train_files, test_files     \n",
    "\n",
    "#### EXPLORATORY DATA ANALYSIS\n",
    "def perform_eda(df, dataset_type):\n",
    "    \n",
    "    '''\n",
    "    Exploratory data analysis. \n",
    "        1. Check for missing values and log the features that contain them \n",
    "        and their type. \n",
    "        2. Visualize the missing values in a heatmap. \n",
    "        3. \n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : Pandas dataframe\n",
    "        Contains the data of interest.\n",
    "    dataset_type : STRING\n",
    "        Corresponds to the type of dataset (raw, preprocessed, etc.).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "\n",
    "    '''\n",
    "\n",
    "    \n",
    "    # Check for missing values\n",
    "    if df.isnull().sum().any():\n",
    "        # Define the image path\n",
    "        path_to_images = c.FetchPaths(c.PROJECTS_PATH, c.PROJECT_NAME).to_images()\n",
    "        # plot the heatmap of the missing values\n",
    "        plotting.visualize_missing_data(df, dataset_type, path_to_images)\n",
    "        # Get the features that contain missing values\n",
    "        features_with_missing_vals = df.isnull().any()[df.isnull().any()].index.tolist()\n",
    "                        \n",
    "        c.logging.critical('!!! There are features with missing values')\n",
    "        c.logging.info(f'Features: {features_with_missing_vals}')\n",
    "        c.logging.info(f'Types: {df.dtypes[features_with_missing_vals]}')\n",
    "        \n",
    "    else:\n",
    "        c.logging.info('None of the features has missing values.')\n",
    "    \n",
    "\n",
    "    # Define the image path\n",
    "    path_to_images = c.FetchPaths(c.PROJECTS_PATH, c.PROJECT_NAME).to_images()\n",
    "\n",
    "\n",
    "    # plot and save the target distribution\n",
    "    plotting.plot_target_distribution(df, dataset_type, path_to_images)\n",
    "    # plot the feature correlation matrix\n",
    "    plotting.plot_feature_correlation(df, dataset_type, path_to_images)\n",
    "    \n",
    "\n",
    "    \n",
    "#### DATA PREPROCESSING\n",
    "def preprocess_data(df, dataset_type, path):\n",
    "\n",
    "    '''\n",
    "    This script preprocesses the data and does: \n",
    "    \n",
    "    1. Nan Imputation for the numerical and categorical values (median & mode)\n",
    "    2. Frequency encoding for the categorical values    \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : Pandas dataframe\n",
    "        Contains the data of interest.\n",
    "    dataset_type : STRING\n",
    "        Corresponds to the type of dataset (raw, preprocessed, etc.).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "\n",
    "    '''\n",
    "\n",
    "    \n",
    "    # Numerical features that contain missing values\n",
    "    num_features = ['AGE','YOJ','CAR_AGE']            \n",
    "    # fill the missing values with the median\n",
    "    for feature in num_features:\n",
    "        df[feature][df[feature].isnull()] = df[feature].median()\n",
    "  \n",
    "    ###########################\n",
    "    # ENCODE CATEGORICAL VALUES   \n",
    "    ############################\n",
    "    # Categorical features that contain missing data\n",
    "    cat_features_with_nans = ['INCOME', 'HOME_VAL', 'JOB']\n",
    "    \n",
    "    # fill the categorical values with the mode\n",
    "    # filling with most common class\n",
    "    df[cat_features_with_nans] = \\\n",
    "        df[cat_features_with_nans].apply(lambda x: x.fillna(x.value_counts().index[0]))\n",
    "\n",
    "    # now select all categorical features and apply frequency encoding\n",
    "    cat_features = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    for cat_feature in cat_features:\n",
    "        # apply frequency encoding to the categorical feature(s)\n",
    "        encoding_value = df.groupby(cat_feature).mean()['AGE']\n",
    "        df[cat_feature] = df[cat_feature].map(encoding_value.to_dict())\n",
    "    \n",
    "    \n",
    "    \n",
    "    # finally, drop the TARGET_AMT feature\n",
    "    df.drop(columns='TARGET_AMT', inplace=True)\n",
    "    \n",
    "    # Save the dataset in the preprocessed folder\n",
    "    path_to_data = path.to_data_preprocessed()\n",
    "    df.to_csv(c.join(path_to_data, f'{dataset_type}_preprocessed.csv'))\n",
    "    \n",
    "    if dataset_type=='train':\n",
    "        ############################\n",
    "        # PREPARE DATA FOR SKLEARN # \n",
    "        ############################\n",
    "        y = df.TARGET_FLAG\n",
    "        # drop the target\n",
    "        df.drop(columns='TARGET_FLAG', inplace=True)\n",
    "        X = df\n",
    "\n",
    "        return X, y\n",
    "    else:\n",
    "        return None\n",
    "        \n",
    "\n",
    "def over_sample_dataset(X,y):\n",
    "    '''\n",
    "    The EDA revieled that the target distribution was skewed, which might\n",
    "    lead to overfitting, to avoid that, we over-sample the missing target class.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : Pandas dataframe\n",
    "        The original feature matrix.\n",
    "    y : Array\n",
    "        The original target array.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_over : Pandas dataframe\n",
    "        The over-sampled feature matrix.\n",
    "    y_over : Array\n",
    "        The over-sampled target array.\n",
    "\n",
    "    '''\n",
    "\n",
    "    print('Original dataset shape %s' % Counter(y))\n",
    "    col = X.columns.values\n",
    "    sm = SMOTE(random_state=c.random_state)\n",
    "    X_df, y_df = sm.fit_resample(X, y)\n",
    "    print('Resampled dataset shape %s' % Counter(y_df))\n",
    "    \n",
    "    X_over = pd.DataFrame(X_df , columns=col)\n",
    "    y_over = pd.DataFrame(y_df)\n",
    "\n",
    "    # return the oversampled dataset\n",
    "    return X_over, y_over\n",
    " \n",
    "\n",
    "def split_data_for_classification(X,y, ratio=0.25):\n",
    "    '''\n",
    "    Split the original dataset using a simple train-test split    \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : Pandas dataframe\n",
    "        The over-sampled feature matrix.\n",
    "    y : Array\n",
    "        The over-sampled target array.\n",
    "    ratio: Float\n",
    "        The ratio of the test set. Defaults to 25%\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    75% of the data to be used for training and 25% for testing\n",
    "\n",
    "    '''    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, \\\n",
    "                                                        test_size=ratio, \n",
    "                                                        stratify=y_over, \n",
    "                                                        random_state=c.random_state) \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def cross_val_hyperparam_tuning(RUN_RANDOMSEARCH, X,y, path):\n",
    "    '''\n",
    "    Perform Randomized search on hyper parameters. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    RUN_RANDOMSEARCH : Bool\n",
    "        Whether to run the randomized search or load previous best params.\n",
    "    X : DF\n",
    "        Feature Matrix.\n",
    "    y : Array\n",
    "        Target.\n",
    "    path : Class\n",
    "        Used to save the best params if RUN_RANDOMSEARCH=False.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    best_params : Dict\n",
    "        Contains the best parameters of the Randomized search on hyper parameters.\n",
    "        If RUN_RANDOMSEARCH == True, the function returns params calculated online, \n",
    "        otherwise, it loads them from a previous search. \n",
    "\n",
    "    '''\n",
    "\n",
    "    path_to_params = path.to_params()    \n",
    "\n",
    "    if RUN_RANDOMSEARCH:\n",
    "    \n",
    "        start_time = time.time()\n",
    "        gkf = StratifiedKFold(n_splits=5, shuffle=True,\n",
    "                              random_state=c.random_state).split(X=X, y=y)\n",
    "        rsearch = RandomizedSearchCV(lgb_estimator, \n",
    "                                     param_distributions=c.param_test,\n",
    "                                     cv=gkf, n_jobs=-1)\n",
    "        lgb_model_random = rsearch.fit(X=X, y=np.ravel(y,order='C'))\n",
    "        \n",
    "        best_params = lgb_model_random.best_params_\n",
    "        best_params[\"objective\"] = \"binary\"\n",
    "        \n",
    "        print(lgb_model_random.best_params_, lgb_model_random.best_score_)\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "        # save the parameters\n",
    "        with open(c.join(path_to_params,'best_params.pkl'), 'wb') as f:\n",
    "            pickle.dump(lgb_model_random.best_params_, f)\n",
    "        \n",
    "        best_params = lgb_model_random.best_params_ \n",
    "    else:\n",
    "\n",
    "        with open(c.join(path_to_params,'best_params.pkl'), 'rb') as f:\n",
    "            best_params = pickle.load(f)\n",
    "\n",
    "        \n",
    "    \n",
    "    return best_params\n",
    "\n",
    "\n",
    "def train_and_evaluate_model(classifier,X_train,y_train,X_test,y_test,\n",
    "                         best_params, path):\n",
    "    '''\n",
    "    Receives the classifier and evaluates the model on the train_files.\n",
    "    Plots the ROC AUC for this dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "\n",
    "    '''\n",
    "    clf = LGBMClassifier(**best_params)\n",
    "    \n",
    "    clf.fit(X_train, np.ravel(y_train,order='C'), eval_metric=\"auc\")\n",
    "    preds_proba_test = clf.predict_proba(X_test)\n",
    "        \n",
    "    # plot the AUC curve\n",
    "    path_to_images = c.FetchPaths(c.PROJECTS_PATH, c.PROJECT_NAME).to_images()\n",
    "    plotting.plot_roc_auc(y_test,preds_proba_test[:, 1], path_to_images)\n",
    "    # save the classification report    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178886fc",
   "metadata": {},
   "source": [
    "# PERFORM INFERENCE\n",
    "\n",
    "The inference task is divided in the following parts. \n",
    "\n",
    "1. Load the raw data provided by the assignment.\n",
    "2. Perform exploratory data analysis on the two assignments to build insights on the nature and the peculiarities of the data.\n",
    "3. Preprocess the data (both datasets). \n",
    "4. Define the classifier\n",
    "5. Optimize the classifier using hyper-parameter tuning. \n",
    "6. Use the \"train_files\" to train and evaluate the performance of the classifier. \n",
    "7. Finally, use the optimized classifier to generalize to the instances of the \"test_files\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981b6192",
   "metadata": {},
   "source": [
    "## GLOBALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7cea47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to false in order to not relaunch the RandomSearch and\n",
    "# use the best hyperparameters that already calculated\n",
    "RUN_RANDOMSEARCH = True\n",
    "    \n",
    "# call the path constructor - Defined @config.ipynb this constructor can be used to create the path derivatives\n",
    "path = c.FetchPaths(c.PROJECTS_PATH, c.PROJECT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0289a63d",
   "metadata": {},
   "source": [
    "## LOAD THE RAW DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a40d519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the raw dataset\n",
    "train_files, test_files  = load_data(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae07d845",
   "metadata": {},
   "source": [
    "We will build a model based on the train_files and then infer the labels on the test_files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2153696",
   "metadata": {},
   "source": [
    "## EXPLORATORY DATA ANALYSIS\n",
    "\n",
    "This part contains the following steps: \n",
    "   \n",
    "   1. <span style='color:blue '>[ARE THERE MISSING VALUES?] </span> To answer this, we plot a heatmap that allows us to observe the missing value per feature for both datasets.\n",
    "   We observe that both datasets contain features with missing values, but none of the features has the majority of the values missing, to justify the exclusion of this feature. We will therefore proceed with data imputation. This is not the case of the feature \"TARGET_AMT\", which is empty in the \"test_files\" but not in the \"train_files\". We don't have any additionall information on this feature. We therefore drop this from both datasets (\"train\" and \"test\"). Additionally, this part of the code logs the features that contain missing values, as well as their type.<br><br>\n",
    "   \n",
    "   2. <span style='color:blue '> [HOW ARE THE TARGET LABELS DISTRIBUTED?] </span> To answer this, we plot the distribution of the labels for the target. We observed that, the distribution is skewed. There are way more \"1\" elements compared to \"0\". This indicates that we cannot use metrics such as accuracy or F1, <span style='color:green '> and we should rather use AUC or log-loss. </span> <br><br>\n",
    "   \n",
    "   3. <span style='color:blue '> [ARE THERE HIGH CORRELATED FEATURES?] </span> To answer this, we plot the correlation plot of the features (Spearman correlation). We do not observe any highly correlated features. We will therefore proceed <span style='color:green '> without applying any dimentionality reduction technique. </span> <br><br>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925c27b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_eda(train_files, 'train')\n",
    "perform_eda(test_files, 'test') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4646907",
   "metadata": {},
   "source": [
    "## PREPROCESSING\n",
    "\n",
    "Based on the observations of the EDA, we decided on the following preprocessing steps: \n",
    "\n",
    "1. <span style='color:red '>[DATA IMPUTATION] </span> The numerical missing values where set to their respective median, whereas for the categorical values, we used the mode. <br><br>\n",
    "2. <span style='color:red '>[FREQUENCY ENCODING] </span> After imputing the missing values, we transformed the categorical features into numerical by applying frequency encoding. The preprocessed data (up to this stage) were stored into a separate directory. <br><br>\n",
    "3. <span style='color:red '>[OVERSAMPLING] </span> To balance the target of the \"train\" dataset we also performed oversampling of the minority class, and used this dataset to tune our model. The alternative to that would have been a stratified K-fold, but this will take significantly more time to complete. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b47bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESSING\n",
    "# We will use the train dataset to build a model\n",
    "X, y = preprocess_data(train_files, 'train', path)\n",
    "# we perform preprocessing and save the data that will be used for the inference\n",
    "preprocess_data(test_files, 'test', path)\n",
    "# ovesample due to the target imbalance\n",
    "X_over, y_over = over_sample_dataset(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf9f55d",
   "metadata": {},
   "source": [
    "## CLASSIFICATION USING LightGBM\n",
    "\n",
    "For our task, we use the Light Gradient Boosting Machine, which is a fast and efficient implementation of a gradient boosting algorithm, currently, the state-of-the-art in classification problems. The light implementation of this model allows us to directly implement it in our framework, thus bypassing the exploration with simple linear models (such as SVM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08de4515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data to classify\n",
    "X_train, X_test, y_train, y_test = split_data_for_classification(X_over,y_over, ratio=0.25)\n",
    "\n",
    "# CLASSIFICATION USING LightGBM\n",
    "lgb_estimator = LGBMClassifier(\n",
    "    boosting_type=\"gbdt\", objective=\"binary\", learning_rate=0.01, metric=\"auc\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd38e166",
   "metadata": {},
   "source": [
    "## RANDOMIZED SEARCH ON HYPER PARAMETERS\n",
    "\n",
    "To optimize for execution time, we used a randomized search for hyper-parameter tuning and not a grid-search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bb9ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter search with Cross Validation\n",
    "best_params = cross_val_hyperparam_tuning(RUN_RANDOMSEARCH,\n",
    "                                          X_train,y_train, path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609de868",
   "metadata": {},
   "source": [
    "## TRAIN & EVALUATE THE MODEL ON THE \"TRAIN FILES\"\n",
    "\n",
    "The performance of the model was evaluated on the \"train_files\". The model achieved high performance (AUC = 0.93). We are thus confident that we can use this model for generalization to new instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c74188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN AND EVALUATE THE CLASSIFIER ON THE TRAIN FILES\n",
    "train_and_evaluate_model(lgb_estimator,X_train,y_train,X_test,y_test,\n",
    "                         best_params, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d063dac",
   "metadata": {},
   "source": [
    "## USE THE OPTIMIZED MODEL TO PREDICT THE CLASS OF THE \"TEST FILES\"\n",
    "\n",
    "Lastly, the tuned model was trained again using the whole \"train_files\" dataset and generated predictions for the target of the \"test_files\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c5a723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, train a classifier on the whole dataset to predict the classes on \n",
    "# the test dataset\n",
    "clf = LGBMClassifier(**best_params)\n",
    "clf.fit(X, y)\n",
    "X_of_interest = test_files.drop(columns = 'TARGET_FLAG')\n",
    "y_predicted = clf.predict(X_of_interest)\n",
    "\n",
    "# save the predicted classes in the predictions folder\n",
    "np.savetxt(c.join(path.to_results(),'predicted_classes.csv'), \n",
    "           y_predicted, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9db717",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
